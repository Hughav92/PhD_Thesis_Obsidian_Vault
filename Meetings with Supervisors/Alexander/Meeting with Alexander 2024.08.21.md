#supervision #Alexander

1. We talk a lot about multimodality when discussing data types (i.e. data modalities), but it is also useful to think about things from a cross-modal perspective, i.e. when separate modalities support, reinforce, or contradict one another. This is a perspective from psychology, so I should also look into this as a possible way to frame discussions too. #cross-modal #multimodal
   
2. Clear idea for a way of framing the project - the [[Timeline|timeline]]. Maybe focus the project just on time-series data modalities. In this sense, we have an underlying (musical) phenomenon that takes place in "analogue" time, captured at specific intervals in specific ways across data modalities. In terms of analysis, we decouple the data from it's specific time indexing, have a separate timeline and fit the data to it. A similar idea was mentioned by [[Atrey et al_2010_Multimodal fusion for multimedia analysis.pdf|(Atrey et al., 2010, p. 346)]]
   
   > “When to fuse? The time when the fusion should take place is an important consideration in the multimodal fusion process. Certain characteristics of media, such as varying data capture rates and processing time of the media, poses challenges on how to synchronize the overall process of fusion. Often this has been addressed by performing the multimedia analysis tasks (such as event detection) over a timeline [29]. A timeline refers to a measurable span of time with information denoted at designated points. The timeline-based accomplishment of a task requires identification of designated points at which fusion of data or information should take place. Due to the asynchrony and diversity among streams and due to the fact that different analysis tasks are performed at different granularity levels in time, the identification of these designated points, i.e. when the fusion should take place, is a challenging issue [8].”
   
   [29] refers to:
   
   > Chieu, H.L., Lee, Y.K.: Query based event extraction along a timeline. In: International ACM Conference on Research and Development in Information Retrieval, pp. 425–432. Sheffield (2004)
   
   [8] refers to:

>	Atrey, P.K., Kankanhalli, M.S., Jain, R.: Information assimilation framework for event detection in multimedia surveillance systems. Springer/ACM Multimed. Syst. J. 12(3), 239–253 (2006)
   
   I should look into this. #timeline
   
3. Thinking about difficulties in framing. There are three separate key structures in the framing:
   
	1. The multimodal, multimedia etc. stuff
	2. The data fusion, sensor fusion stuff
	3. The data, signals, time-series stuff
	   
	All of these are quite nebulous terms depending upon the discipline/sub-discipline. And part of the job is to define a structure and model to navigate this for the review paper. #definitions #multimodal #fusion #data #signals #time-series #framing
	
4.  Frame the project as a data fusion approach to the challenges of multimodality. Focus on the temporality of data, the "gaps" between the samples and relating this to the perceptual qualities of the timeline of the phenomenon. #framing #perception

5. Advice to Alexander from Rolf Inge Godøy - Write at least 15 minutes every day. So write all the time: notes, parts of texts, ideas #writing #advice
   
6. Mentally split the project up into years. Be more free in the first year, especially in relation to the exploratory nature of the project. #advice #time-planning
   
7. Make sure to develop an overall structure to the project so that it can be packaged up. Master and Bachelor students will become involved, so make sure that there is something that I can give them to do. So that I can practice being in a supervisory role. #project-structure 

8. Develop structure for everything: note taking, writing, references. Everything. #project-structure 

Next meeting on [[Meeting with Alexander 2024.09.05|05.09.2024]].
